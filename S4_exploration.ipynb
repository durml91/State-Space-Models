{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTqA2HWqnANoqgXexctuka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durml91/State-Space-Models/blob/main/S4_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "S4 exploration\n",
        "---\n",
        "Taken from https://srush.github.io/annotated-s4/. Alternative approach to attention-only models (see Mamba next). Big problem with attention is quadratic scaling with longer and longer context windows (hence solutions such as sliding window attention). Key ideas involves long range sequence modelling, continuous vs. discrete (think neural ODEs) and CNN at training to RNN at inference (convolution is an operation that mixes information across vectors, analagous to attention).\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOLcMZo1PrdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install equinox"
      ],
      "metadata": {
        "id": "xuFzyaKfRTYy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m5YkLia5PTBX"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jr\n",
        "import equinox"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = jr.PRNGKey(2024)"
      ],
      "metadata": {
        "id": "g6GKPnmeRhC8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "State space model: $x'(t) = \\boldsymbol{A} x(t) + \\boldsymbol{B} u(t)$ and $y(t) = \\boldsymbol{C} x(t) + \\boldsymbol{D} u(t)$. This is an ODE w.r.t. time whereby the input is actually $u(t)$ in 1D, $x(t)$ is multi-dimensional and the output \"signal\" $y(t)$ is 1D. The parameters/matrices in bold are learnt, although the authors adopt the convention $\\boldsymbol{D}=0$ as this bit is actually just a skip connection."
      ],
      "metadata": {
        "id": "23zwCFFrRrmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initiliase random matrices\n",
        "\n",
        "def random_params(N, key):\n",
        "  a_key, b_key, c_key = jr.split(key, 3)\n",
        "\n",
        "  A = jr.uniform(key=a_key, shape=(N,N))\n",
        "  B = jr.uniform(key=b_key, shape=(N,1))\n",
        "  C = jr.uniform(key=c_key, shape=(1,N))\n",
        "\n",
        "  return A,B,C"
      ],
      "metadata": {
        "id": "TWXDZ1TuRkjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discretise input sequence $u(t) \\to (u_{0}, u_{1},...)$. So we are basically sampling from the underlying signal every step of size $\\Delta$ such that $u_{i} = u(i \\cdot \\Delta)$. The authors use the bilinear method from control theory (continuous time in Laplace domain to discrete time in complex plane) to approximate the parameters: $\\bar{\\boldsymbol{A}} = (\\boldsymbol{I} - \\frac{\\Delta}{2} \\cdot \\boldsymbol{A})^{-1}(\\boldsymbol{I} + \\frac{\\Delta}{2} \\cdot \\boldsymbol{A})$, $\\bar{\\boldsymbol{B}} = (\\boldsymbol{I} - \\frac{\\Delta}{2} \\cdot \\boldsymbol{A})^{-1} \\Delta \\boldsymbol{B}$ and $\\bar{\\boldsymbol{C}}= \\boldsymbol{C}$. This discretisation (analagous to the Euler discretisation) allows us to write the ODE as a recurrence relation, namely $x_k = \\bar{\\boldsymbol{A}}x_{k-1} + \\bar{\\boldsymbol{B}}u_k$. Given $\\boldsymbol{D} = 0$ (sort of), this also gives us a nice closed form solution for y, namely $y_k = \\bar{\\boldsymbol{C}} (\\bar{\\boldsymbol{A}}x_{k-1} + \\bar{\\boldsymbol{B}}u_k)$. This looks like an RNN layer."
      ],
      "metadata": {
        "id": "wZmzosLXUco_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compute bilinear transformation\n",
        "def discretise(A, B, C, step):\n",
        "  I = jnp.eye(A.shape[0])\n",
        "  bl = jnp.linalg.inv(I - (step / 2.) * A)\n",
        "  A_bl = bl @ (I + (step / 2.) * A)\n",
        "  B_bl = (bl * step) @ B\n",
        "\n",
        "  return A_bl, B_bl, C\n",
        "\n",
        "def scan_SSM(A_bl, B_bl, C_bl, u, x0):\n",
        "  def step(x_k_1, u_k):\n",
        "    x_k = A_bl @ x_k_1 + B_bl @ u_k\n",
        "    y_k = C_bl @ x_k\n",
        "    return x_k, y_k\n",
        "\n",
        "  return jax.lax.scan(step, x0, u) # basically apply step with x0 as carry and u as invariant\n",
        "\n",
        "def run_SSM(A, B, C, u):\n",
        "  L= u.shape[0]\n",
        "  N = A.shape[0]\n",
        "  A_bl, B_bl, C_bl = discretise(A, B, C, step=1./L)\n",
        "\n",
        "  return scan_SSM(A_bl, B_bl, C_bl, jnp.expand_dims(u, axis=-1))"
      ],
      "metadata": {
        "id": "HnR6o1PWTnz0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}